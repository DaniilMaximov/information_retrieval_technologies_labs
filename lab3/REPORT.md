# Отчет по лабораторной 3

## Описание работы модели

Модель использует **эмбеддинги слов и предложений** для поиска запросов, которые семантически наиболее близки к входному поисковому запросу. Это достигается путём вычисления **косинусного сходства** между векторами (эмбеддингами) запросов. Чем выше сходство, тем более релевантен найденный запрос.

Эмбеддинги для слов и текстов получены с использованием предобученной модели Navec. Модель работает следующим образом:

1. Текст запроса преобразуется в эмбеддинг (вектор).
2. Эмбеддинги всех запросов в базе данных уже предварительно рассчитаны.
3. Для нового запроса ищутся топ-N запросов из базы, которые имеют наибольшее косинусное сходство с входным запросом.

---

## Функция для поиска похожих запросов

Ниже приведена функция, которая принимает поисковой запрос и возвращает список похожих запросов:

```python
def get_similar_queries(query, embedded_dataset, original_dataset, top_n=5):
    """
    Находит топ-N похожих запросов из базы данных для заданного поискового запроса.

    :param query: текстовый запрос пользователя
    :param embedded_dataset: список эмбеддингов всех запросов в базе
    :param original_dataset: список всех запросов в текстовом виде
    :param top_n: количество возвращаемых похожих запросов
    :return: список текстов топ-N похожих запросов
    """
    query_embedding = embed(query)  # Создаем эмбеддинг для запроса
    similarities = [
        cosine(query_embedding, emb) for emb in embedded_dataset
    ]  # Считаем косинусное сходство с каждым запросом в базе
    top_indices = np.argsort(similarities)[-top_n:][::-1]  # Берём индексы топ-N запросов
    return [original_dataset[i] for i in top_indices]



## Результаты тестирования модели

Ниже представлены результаты тестирования модели на 10 запросах с вычислением средней семантической близости.

### Таблица результатов

| **Запрос**                     | **Средняя семантическая близость** | **Комментарии**                                                                                          |
|--------------------------------|------------------------------------|----------------------------------------------------------------------------------------------------------|
| Как начать программировать?    | 0.4958                             | Модель предложила частично релевантные запросы, но присутствуют нерелевантные, например, "как начать биткойн". |
| Лучшие книги по истории         | 0.8010                             | Высокая точность, большинство предложенных запросов релевантны и совпадают с темой.                      |
| Как приготовить борщ?          | 0.5740                             | Результаты в основном связаны с готовкой, но конкретно борщ не упоминается.                              |
| Что делать при простуде?       | 0.5247                             | Присутствуют нерелевантные запросы, такие как "почему cse при nsit лучше, чем cse при dtu".               |
| Где купить автомобиль?         | 0.6959                             | В основном релевантные запросы, но некоторые слишком общие, например, "сколько стоит купить автомобиль". |
| Лучшие смартфоны 2024 года     | 0.6262                             | Модель возвращает запросы с другими годами (2016), но сохраняет тематику смартфонов.                     |
| Как улучшить свои навыки общения?| 0.5609                            | Релевантные результаты, но недостаточное разнообразие: запросы почти идентичны.                          |
| Какие фильмы посмотреть на выходных?| 0.6169                        | В основном релевантные, но часть запросов отклоняется от темы фильмов для выходных.                      |
| Как найти работу программистом?| 0.4881                             | Некоторые запросы связаны с работой, но нет фокуса на профессии программиста.                            |
| Советы для путешествий         | 0.4990                             | Результаты частично релевантны, но присутствуют нерелевантные запросы, такие как советы для экзаменов GRE. |

**Средний коэффициент семантической близости по всем запросам: 0.5883**

---
